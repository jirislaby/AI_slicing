From ad487df131c6ae2652ac3fd540df225044b48c7b Mon Sep 17 00:00:00 2001
From: Jiri Slaby <jslaby@suse.cz>
Date: Wed, 26 Sep 2018 09:41:32 +0200
Subject: [PATCH 1/1] symbolic execution

Based on 234b69e3e089d850a98e7b3145bd00e9b52b1111.
---
 arch/x86/Makefile                |   2 +-
 arch/x86/include/asm/atomic.h    |  17 +-
 arch/x86/include/asm/bitops.h    | 261 ++++++++--------
 arch/x86/include/asm/bug.h       |   7 +-
 arch/x86/include/asm/current.h   |   2 +-
 arch/x86/include/asm/irqflags.h  |  29 +-
 arch/x86/include/asm/page.h      |   1 +
 arch/x86/include/asm/page_64.h   |   8 +-
 arch/x86/include/asm/preempt.h   |  20 +-
 arch/x86/include/asm/processor.h |   6 -
 arch/x86/include/asm/uaccess.h   |  12 +-
 arch/x86/include/uapi/asm/swab.h |  18 +-
 fs/btrfs/Makefile                |   2 +-
 fs/btrfs/disk-io.c               |   4 +-
 fs/btrfs/main.c                  | 502 +++++++++++++++++++++++++++++++
 fs/btrfs/raid56.c                |   4 +-
 fs/btrfs/super.c                 |   6 +-
 fs/btrfs/volumes.c               |   2 +-
 fs/ext4/Makefile                 |   3 +-
 fs/ext4/main.c                   | 131 ++++++++
 fs/ext4/super.c                  |   2 +-
 fs/inode.c                       |  11 +-
 include/linux/rwlock.h           |  30 +-
 include/linux/spinlock.h         |  11 +-
 24 files changed, 846 insertions(+), 245 deletions(-)
 create mode 100644 fs/btrfs/main.c
 create mode 100644 fs/ext4/main.c

diff --git a/arch/x86/Makefile b/arch/x86/Makefile
index 8f6e7eb8ae9f..d5f45f1fc5be 100644
--- a/arch/x86/Makefile
+++ b/arch/x86/Makefile
@@ -300,7 +300,7 @@ archprepare: checkbin
 checkbin:
 ifndef CC_HAVE_ASM_GOTO
 	@echo Compiler lacks asm-goto support.
-	@exit 1
+	#@exit 1
 endif
 
 archclean:
diff --git a/arch/x86/include/asm/atomic.h b/arch/x86/include/asm/atomic.h
index ce84388e540c..a07849e4617b 100644
--- a/arch/x86/include/asm/atomic.h
+++ b/arch/x86/include/asm/atomic.h
@@ -94,8 +94,7 @@ static __always_inline bool arch_atomic_sub_and_test(int i, atomic_t *v)
  */
 static __always_inline void arch_atomic_inc(atomic_t *v)
 {
-	asm volatile(LOCK_PREFIX "incl %0"
-		     : "+m" (v->counter));
+	++v->counter;
 }
 #define arch_atomic_inc arch_atomic_inc
 
@@ -107,8 +106,7 @@ static __always_inline void arch_atomic_inc(atomic_t *v)
  */
 static __always_inline void arch_atomic_dec(atomic_t *v)
 {
-	asm volatile(LOCK_PREFIX "decl %0"
-		     : "+m" (v->counter));
+	--v->counter;
 }
 #define arch_atomic_dec arch_atomic_dec
 
@@ -122,7 +120,7 @@ static __always_inline void arch_atomic_dec(atomic_t *v)
  */
 static __always_inline bool arch_atomic_dec_and_test(atomic_t *v)
 {
-	GEN_UNARY_RMWcc(LOCK_PREFIX "decl", v->counter, "%0", e);
+	return --v->counter == 0;
 }
 #define arch_atomic_dec_and_test arch_atomic_dec_and_test
 
@@ -191,7 +189,10 @@ static __always_inline int arch_atomic_fetch_sub(int i, atomic_t *v)
 
 static __always_inline int arch_atomic_cmpxchg(atomic_t *v, int old, int new)
 {
-	return arch_cmpxchg(&v->counter, old, new);
+	int mold = v->counter;
+	if (mold == old)
+		v->counter = new;
+	return mold;
 }
 
 #define arch_atomic_try_cmpxchg arch_atomic_try_cmpxchg
@@ -202,7 +203,9 @@ static __always_inline bool arch_atomic_try_cmpxchg(atomic_t *v, int *old, int n
 
 static inline int arch_atomic_xchg(atomic_t *v, int new)
 {
-	return arch_xchg(&v->counter, new);
+	int old = v->counter;
+	v->counter = new;
+	return old;
 }
 
 static inline void arch_atomic_and(int i, atomic_t *v)
diff --git a/arch/x86/include/asm/bitops.h b/arch/x86/include/asm/bitops.h
index 9f645ba57dbb..3c7b1180d02a 100644
--- a/arch/x86/include/asm/bitops.h
+++ b/arch/x86/include/asm/bitops.h
@@ -54,6 +54,23 @@
 #define CONST_MASK_ADDR(nr, addr)	BITOP_ADDR((void *)(addr) + ((nr)>>3))
 #define CONST_MASK(nr)			(1 << ((nr) & 7))
 
+/**
+ * __set_bit - Set a bit in memory
+ * @nr: the bit to set
+ * @addr: the address to start counting from
+ *
+ * Unlike set_bit(), this function is non-atomic and may be reordered.
+ * If it's called on the same region of memory simultaneously, the effect
+ * may be that only one operation succeeds.
+ */
+static __always_inline void __set_bit(long nr, volatile unsigned long *addr)
+{
+	unsigned long mask = BIT_MASK(nr);
+	unsigned long *p = ((unsigned long *)addr) + BIT_WORD(nr);
+
+	*p |= mask;
+}
+
 /**
  * set_bit - Atomically set a bit in memory
  * @nr: the bit to set
@@ -72,29 +89,15 @@
 static __always_inline void
 set_bit(long nr, volatile unsigned long *addr)
 {
-	if (IS_IMMEDIATE(nr)) {
-		asm volatile(LOCK_PREFIX "orb %1,%0"
-			: CONST_MASK_ADDR(nr, addr)
-			: "iq" ((u8)CONST_MASK(nr))
-			: "memory");
-	} else {
-		asm volatile(LOCK_PREFIX __ASM_SIZE(bts) " %1,%0"
-			: BITOP_ADDR(addr) : "Ir" (nr) : "memory");
-	}
+	__set_bit(nr, addr);
 }
 
-/**
- * __set_bit - Set a bit in memory
- * @nr: the bit to set
- * @addr: the address to start counting from
- *
- * Unlike set_bit(), this function is non-atomic and may be reordered.
- * If it's called on the same region of memory simultaneously, the effect
- * may be that only one operation succeeds.
- */
-static __always_inline void __set_bit(long nr, volatile unsigned long *addr)
+static __always_inline void __clear_bit(long nr, volatile unsigned long *addr)
 {
-	asm volatile(__ASM_SIZE(bts) " %1,%0" : ADDR : "Ir" (nr) : "memory");
+	unsigned long mask = BIT_MASK(nr);
+	unsigned long *p = ((unsigned long *)addr) + BIT_WORD(nr);
+
+	*p &= ~mask;
 }
 
 /**
@@ -110,15 +113,7 @@ static __always_inline void __set_bit(long nr, volatile unsigned long *addr)
 static __always_inline void
 clear_bit(long nr, volatile unsigned long *addr)
 {
-	if (IS_IMMEDIATE(nr)) {
-		asm volatile(LOCK_PREFIX "andb %1,%0"
-			: CONST_MASK_ADDR(nr, addr)
-			: "iq" ((u8)~CONST_MASK(nr)));
-	} else {
-		asm volatile(LOCK_PREFIX __ASM_SIZE(btr) " %1,%0"
-			: BITOP_ADDR(addr)
-			: "Ir" (nr));
-	}
+	__clear_bit(nr, addr);
 }
 
 /*
@@ -135,19 +130,14 @@ static __always_inline void clear_bit_unlock(long nr, volatile unsigned long *ad
 	clear_bit(nr, addr);
 }
 
-static __always_inline void __clear_bit(long nr, volatile unsigned long *addr)
-{
-	asm volatile(__ASM_SIZE(btr) " %1,%0" : ADDR : "Ir" (nr));
-}
-
 static __always_inline bool clear_bit_unlock_is_negative_byte(long nr, volatile unsigned long *addr)
 {
-	bool negative;
-	asm volatile(LOCK_PREFIX "andb %2,%1"
-		CC_SET(s)
-		: CC_OUT(s) (negative), ADDR
-		: "ir" ((char) ~(1 << nr)) : "memory");
-	return negative;
+	long old;
+
+	addr += BIT_WORD(nr);
+	old = *addr;
+	clear_bit(nr, addr);
+	return !!(old & BIT(7));
 }
 
 // Let everybody know we have it
@@ -182,7 +172,10 @@ static __always_inline void __clear_bit_unlock(long nr, volatile unsigned long *
  */
 static __always_inline void __change_bit(long nr, volatile unsigned long *addr)
 {
-	asm volatile(__ASM_SIZE(btc) " %1,%0" : ADDR : "Ir" (nr));
+	unsigned long mask = BIT_MASK(nr);
+	unsigned long *p = ((unsigned long *)addr) + BIT_WORD(nr);
+
+	*p ^= mask;
 }
 
 /**
@@ -196,15 +189,26 @@ static __always_inline void __change_bit(long nr, volatile unsigned long *addr)
  */
 static __always_inline void change_bit(long nr, volatile unsigned long *addr)
 {
-	if (IS_IMMEDIATE(nr)) {
-		asm volatile(LOCK_PREFIX "xorb %1,%0"
-			: CONST_MASK_ADDR(nr, addr)
-			: "iq" ((u8)CONST_MASK(nr)));
-	} else {
-		asm volatile(LOCK_PREFIX __ASM_SIZE(btc) " %1,%0"
-			: BITOP_ADDR(addr)
-			: "Ir" (nr));
-	}
+	__change_bit(nr, addr);
+}
+
+/**
+ * __test_and_set_bit - Set a bit and return its old value
+ * @nr: Bit to set
+ * @addr: Address to count from
+ *
+ * This operation is non-atomic and can be reordered.
+ * If two examples of this operation race, one can appear to succeed
+ * but actually fail.  You must protect multiple accesses with a lock.
+ */
+static __always_inline int __test_and_set_bit(long nr, volatile unsigned long *addr)
+{
+	unsigned long mask = BIT_MASK(nr);
+	unsigned long *p = ((unsigned long *)addr) + BIT_WORD(nr);
+	unsigned long old = *p;
+
+	*p = old | mask;
+	return (old & mask) != 0;
 }
 
 /**
@@ -217,8 +221,7 @@ static __always_inline void change_bit(long nr, volatile unsigned long *addr)
  */
 static __always_inline bool test_and_set_bit(long nr, volatile unsigned long *addr)
 {
-	GEN_BINARY_RMWcc(LOCK_PREFIX __ASM_SIZE(bts),
-	                 *addr, "Ir", nr, "%0", c);
+	return __test_and_set_bit(nr, addr);
 }
 
 /**
@@ -234,24 +237,31 @@ test_and_set_bit_lock(long nr, volatile unsigned long *addr)
 	return test_and_set_bit(nr, addr);
 }
 
+
 /**
- * __test_and_set_bit - Set a bit and return its old value
- * @nr: Bit to set
+ * __test_and_clear_bit - Clear a bit and return its old value
+ * @nr: Bit to clear
  * @addr: Address to count from
  *
  * This operation is non-atomic and can be reordered.
  * If two examples of this operation race, one can appear to succeed
  * but actually fail.  You must protect multiple accesses with a lock.
+ *
+ * Note: the operation is performed atomically with respect to
+ * the local CPU, but not other CPUs. Portable code should not
+ * rely on this behaviour.
+ * KVM relies on this behaviour on x86 for modifying memory that is also
+ * accessed from a hypervisor on the same CPU if running in a VM: don't change
+ * this without also updating arch/x86/kernel/kvm.c
  */
-static __always_inline bool __test_and_set_bit(long nr, volatile unsigned long *addr)
+static __always_inline int __test_and_clear_bit(long nr, volatile unsigned long *addr)
 {
-	bool oldbit;
+	unsigned long mask = BIT_MASK(nr);
+	unsigned long *p = ((unsigned long *)addr) + BIT_WORD(nr);
+	unsigned long old = *p;
 
-	asm(__ASM_SIZE(bts) " %2,%1"
-	    CC_SET(c)
-	    : CC_OUT(c) (oldbit), ADDR
-	    : "Ir" (nr));
-	return oldbit;
+	*p = old & ~mask;
+	return (old & mask) != 0;
 }
 
 /**
@@ -264,48 +274,18 @@ static __always_inline bool __test_and_set_bit(long nr, volatile unsigned long *
  */
 static __always_inline bool test_and_clear_bit(long nr, volatile unsigned long *addr)
 {
-	GEN_BINARY_RMWcc(LOCK_PREFIX __ASM_SIZE(btr),
-	                 *addr, "Ir", nr, "%0", c);
-}
-
-/**
- * __test_and_clear_bit - Clear a bit and return its old value
- * @nr: Bit to clear
- * @addr: Address to count from
- *
- * This operation is non-atomic and can be reordered.
- * If two examples of this operation race, one can appear to succeed
- * but actually fail.  You must protect multiple accesses with a lock.
- *
- * Note: the operation is performed atomically with respect to
- * the local CPU, but not other CPUs. Portable code should not
- * rely on this behaviour.
- * KVM relies on this behaviour on x86 for modifying memory that is also
- * accessed from a hypervisor on the same CPU if running in a VM: don't change
- * this without also updating arch/x86/kernel/kvm.c
- */
-static __always_inline bool __test_and_clear_bit(long nr, volatile unsigned long *addr)
-{
-	bool oldbit;
-
-	asm volatile(__ASM_SIZE(btr) " %2,%1"
-		     CC_SET(c)
-		     : CC_OUT(c) (oldbit), ADDR
-		     : "Ir" (nr));
-	return oldbit;
+	return __test_and_clear_bit(nr, addr);
 }
 
 /* WARNING: non atomic and it can be reordered! */
 static __always_inline bool __test_and_change_bit(long nr, volatile unsigned long *addr)
 {
-	bool oldbit;
+	unsigned long mask = BIT_MASK(nr);
+	unsigned long *p = ((unsigned long *)addr) + BIT_WORD(nr);
+	unsigned long old = *p;
 
-	asm volatile(__ASM_SIZE(btc) " %2,%1"
-		     CC_SET(c)
-		     : CC_OUT(c) (oldbit), ADDR
-		     : "Ir" (nr) : "memory");
-
-	return oldbit;
+	*p = old ^ mask;
+	return (old & mask) != 0;
 }
 
 /**
@@ -318,8 +298,7 @@ static __always_inline bool __test_and_change_bit(long nr, volatile unsigned lon
  */
 static __always_inline bool test_and_change_bit(long nr, volatile unsigned long *addr)
 {
-	GEN_BINARY_RMWcc(LOCK_PREFIX __ASM_SIZE(btc),
-	                 *addr, "Ir", nr, "%0", c);
+	return __test_and_change_bit(nr, addr);
 }
 
 static __always_inline bool constant_test_bit(long nr, const volatile unsigned long *addr)
@@ -330,14 +309,7 @@ static __always_inline bool constant_test_bit(long nr, const volatile unsigned l
 
 static __always_inline bool variable_test_bit(long nr, volatile const unsigned long *addr)
 {
-	bool oldbit;
-
-	asm volatile(__ASM_SIZE(bt) " %2,%1"
-		     CC_SET(c)
-		     : CC_OUT(c) (oldbit)
-		     : "m" (*(unsigned long *)addr), "Ir" (nr));
-
-	return oldbit;
+	return constant_test_bit(nr, addr);
 }
 
 #if 0 /* Fool kernel-doc since it doesn't do macros yet */
@@ -362,10 +334,33 @@ static bool test_bit(int nr, const volatile unsigned long *addr);
  */
 static __always_inline unsigned long __ffs(unsigned long word)
 {
-	asm("rep; bsf %1,%0"
-		: "=r" (word)
-		: "rm" (word));
-	return word;
+	int num = 0;
+
+#if BITS_PER_LONG == 64
+	if ((word & 0xffffffff) == 0) {
+		num += 32;
+		word >>= 32;
+	}
+#endif
+	if ((word & 0xffff) == 0) {
+		num += 16;
+		word >>= 16;
+	}
+	if ((word & 0xff) == 0) {
+		num += 8;
+		word >>= 8;
+	}
+	if ((word & 0xf) == 0) {
+		num += 4;
+		word >>= 4;
+	}
+	if ((word & 0x3) == 0) {
+		num += 2;
+		word >>= 2;
+	}
+	if ((word & 0x1) == 0)
+		num += 1;
+	return num;
 }
 
 /**
@@ -390,10 +385,33 @@ static __always_inline unsigned long ffz(unsigned long word)
  */
 static __always_inline unsigned long __fls(unsigned long word)
 {
-	asm("bsr %1,%0"
-	    : "=r" (word)
-	    : "rm" (word));
-	return word;
+	int num = BITS_PER_LONG - 1;
+
+#if BITS_PER_LONG == 64
+	if (!(word & (~0ul << 32))) {
+		num -= 32;
+		word <<= 32;
+	}
+#endif
+	if (!(word & (~0ul << (BITS_PER_LONG-16)))) {
+		num -= 16;
+		word <<= 16;
+	}
+	if (!(word & (~0ul << (BITS_PER_LONG-8)))) {
+		num -= 8;
+		word <<= 8;
+	}
+	if (!(word & (~0ul << (BITS_PER_LONG-4)))) {
+		num -= 4;
+		word <<= 4;
+	}
+	if (!(word & (~0ul << (BITS_PER_LONG-2)))) {
+		num -= 2;
+		word <<= 2;
+	}
+	if (!(word & (~0ul << (BITS_PER_LONG-1))))
+		num -= 1;
+	return num;
 }
 
 #undef ADDR
@@ -495,16 +513,9 @@ static __always_inline int fls(int x)
 #ifdef CONFIG_X86_64
 static __always_inline int fls64(__u64 x)
 {
-	int bitpos = -1;
-	/*
-	 * AMD64 says BSRQ won't clobber the dest reg if x==0; Intel64 says the
-	 * dest reg is undefined if x==0, but their CPU architect says its
-	 * value is written to set it to the same as before.
-	 */
-	asm("bsrq %1,%q0"
-	    : "+r" (bitpos)
-	    : "rm" (x));
-	return bitpos + 1;
+	if (x == 0)
+		return 0;
+	return __fls(x) + 1;
 }
 #else
 #include <asm-generic/bitops/fls64.h>
diff --git a/arch/x86/include/asm/bug.h b/arch/x86/include/asm/bug.h
index 6804d6642767..97aa38dfeeb6 100644
--- a/arch/x86/include/asm/bug.h
+++ b/arch/x86/include/asm/bug.h
@@ -67,16 +67,19 @@ do {									\
 
 #endif /* CONFIG_GENERIC_BUG */
 
+extern void __assert_fail(const char *__assertion, const char *__file,
+		unsigned int __line, const char *__function)
+		__attribute__ ((__noreturn__));
 #define HAVE_ARCH_BUG
 #define BUG()							\
 do {								\
-	_BUG_FLAGS(ASM_UD2, 0);					\
+	__assert_fail("BUG", __FILE__, __LINE__, __PRETTY_FUNCTION__); \
 	unreachable();						\
 } while (0)
 
 #define __WARN_FLAGS(flags)					\
 do {								\
-	_BUG_FLAGS(ASM_UD2, BUGFLAG_WARNING|(flags));		\
+	__assert_fail("WARN", __FILE__, __LINE__, __PRETTY_FUNCTION__); \
 	annotate_reachable();					\
 } while (0)
 
diff --git a/arch/x86/include/asm/current.h b/arch/x86/include/asm/current.h
index 3e204e6140b5..d29163730f48 100644
--- a/arch/x86/include/asm/current.h
+++ b/arch/x86/include/asm/current.h
@@ -12,7 +12,7 @@ DECLARE_PER_CPU(struct task_struct *, current_task);
 
 static __always_inline struct task_struct *get_current(void)
 {
-	return this_cpu_read_stable(current_task);
+	return current_task;
 }
 
 #define current get_current()
diff --git a/arch/x86/include/asm/irqflags.h b/arch/x86/include/asm/irqflags.h
index 058e40fed167..e0fb8b3a1693 100644
--- a/arch/x86/include/asm/irqflags.h
+++ b/arch/x86/include/asm/irqflags.h
@@ -14,52 +14,29 @@
  */
 
 /* Declaration required for gcc < 4.9 to prevent -Werror=missing-prototypes */
-extern inline unsigned long native_save_fl(void);
-extern inline unsigned long native_save_fl(void)
+static inline unsigned long native_save_fl(void)
 {
-	unsigned long flags;
-
-	/*
-	 * "=rm" is safe here, because "pop" adjusts the stack before
-	 * it evaluates its effective address -- this is part of the
-	 * documented behavior of the "pop" instruction.
-	 */
-	asm volatile("# __raw_save_flags\n\t"
-		     "pushf ; pop %0"
-		     : "=rm" (flags)
-		     : /* no input */
-		     : "memory");
-
-	return flags;
+	return 0;
 }
 
-extern inline void native_restore_fl(unsigned long flags);
-extern inline void native_restore_fl(unsigned long flags)
+static inline void native_restore_fl(unsigned long flags)
 {
-	asm volatile("push %0 ; popf"
-		     : /* no output */
-		     :"g" (flags)
-		     :"memory", "cc");
 }
 
 static inline void native_irq_disable(void)
 {
-	asm volatile("cli": : :"memory");
 }
 
 static inline void native_irq_enable(void)
 {
-	asm volatile("sti": : :"memory");
 }
 
 static inline __cpuidle void native_safe_halt(void)
 {
-	asm volatile("sti; hlt": : :"memory");
 }
 
 static inline __cpuidle void native_halt(void)
 {
-	asm volatile("hlt": : :"memory");
 }
 
 #endif
diff --git a/arch/x86/include/asm/page.h b/arch/x86/include/asm/page.h
index 7555b48803a8..e0693297fded 100644
--- a/arch/x86/include/asm/page.h
+++ b/arch/x86/include/asm/page.h
@@ -77,6 +77,7 @@ extern bool __virt_addr_valid(unsigned long kaddr);
 #include <asm-generic/getorder.h>
 
 #define HAVE_ARCH_HUGETLB_UNMAPPED_AREA
+#define WANT_PAGE_VIRTUAL 1
 
 #endif	/* __KERNEL__ */
 #endif /* _ASM_X86_PAGE_H */
diff --git a/arch/x86/include/asm/page_64.h b/arch/x86/include/asm/page_64.h
index 939b1cff4a7b..bf299ab2e827 100644
--- a/arch/x86/include/asm/page_64.h
+++ b/arch/x86/include/asm/page_64.h
@@ -46,12 +46,8 @@ void clear_page_erms(void *page);
 
 static inline void clear_page(void *page)
 {
-	alternative_call_2(clear_page_orig,
-			   clear_page_rep, X86_FEATURE_REP_GOOD,
-			   clear_page_erms, X86_FEATURE_ERMS,
-			   "=D" (page),
-			   "0" (page)
-			   : "cc", "memory", "rax", "rcx");
+	void *memset(void *, int, size_t);
+	memset(page, 0, 4096);
 }
 
 void copy_page(void *to, void *from);
diff --git a/arch/x86/include/asm/preempt.h b/arch/x86/include/asm/preempt.h
index 7f2dbd91fc74..a4bfc5458db9 100644
--- a/arch/x86/include/asm/preempt.h
+++ b/arch/x86/include/asm/preempt.h
@@ -20,18 +20,12 @@ DECLARE_PER_CPU(int, __preempt_count);
  */
 static __always_inline int preempt_count(void)
 {
-	return raw_cpu_read_4(__preempt_count) & ~PREEMPT_NEED_RESCHED;
+	return __preempt_count & ~PREEMPT_NEED_RESCHED;
 }
 
 static __always_inline void preempt_count_set(int pc)
 {
-	int old, new;
-
-	do {
-		old = raw_cpu_read_4(__preempt_count);
-		new = (old & PREEMPT_NEED_RESCHED) |
-			(pc & ~PREEMPT_NEED_RESCHED);
-	} while (raw_cpu_cmpxchg_4(__preempt_count, old, new) != old);
+	__preempt_count = pc;
 }
 
 /*
@@ -73,12 +67,12 @@ static __always_inline bool test_preempt_need_resched(void)
 
 static __always_inline void __preempt_count_add(int val)
 {
-	raw_cpu_add_4(__preempt_count, val);
+	__preempt_count += val;
 }
 
 static __always_inline void __preempt_count_sub(int val)
 {
-	raw_cpu_add_4(__preempt_count, -val);
+	__preempt_count -= val;
 }
 
 /*
@@ -101,13 +95,11 @@ static __always_inline bool should_resched(int preempt_offset)
 
 #ifdef CONFIG_PREEMPT
   extern asmlinkage void ___preempt_schedule(void);
-# define __preempt_schedule() \
-	asm volatile ("call ___preempt_schedule" : ASM_CALL_CONSTRAINT)
+# define __preempt_schedule() do { } while (0)
 
   extern asmlinkage void preempt_schedule(void);
   extern asmlinkage void ___preempt_schedule_notrace(void);
-# define __preempt_schedule_notrace() \
-	asm volatile ("call ___preempt_schedule_notrace" : ASM_CALL_CONSTRAINT)
+# define __preempt_schedule_notrace() do { } while (0)
 
   extern asmlinkage void preempt_schedule_notrace(void);
 #endif
diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 222e6a7e7595..1c60da883683 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -801,9 +801,6 @@ extern char			ignore_fpu_irq;
  */
 static inline void prefetch(const void *x)
 {
-	alternative_input(BASE_PREFETCH, "prefetchnta %P1",
-			  X86_FEATURE_XMM,
-			  "m" (*(const char *)x));
 }
 
 /*
@@ -813,9 +810,6 @@ static inline void prefetch(const void *x)
  */
 static inline void prefetchw(const void *x)
 {
-	alternative_input(BASE_PREFETCH, "prefetchw %P1",
-			  X86_FEATURE_3DNOWPREFETCH,
-			  "m" (*(const char *)x));
 }
 
 static inline void spin_lock_prefetch(const void *x)
diff --git a/arch/x86/include/asm/uaccess.h b/arch/x86/include/asm/uaccess.h
index b5e58cc0c5e7..db8e6e494b85 100644
--- a/arch/x86/include/asm/uaccess.h
+++ b/arch/x86/include/asm/uaccess.h
@@ -170,16 +170,8 @@ __typeof__(__builtin_choose_expr(sizeof(x) > sizeof(0UL), 0ULL, 0UL))
  */
 #define get_user(x, ptr)						\
 ({									\
-	int __ret_gu;							\
-	register __inttype(*(ptr)) __val_gu asm("%"_ASM_DX);		\
-	__chk_user_ptr(ptr);						\
-	might_fault();							\
-	asm volatile("call __get_user_%P4"				\
-		     : "=a" (__ret_gu), "=r" (__val_gu),		\
-			ASM_CALL_CONSTRAINT				\
-		     : "0" (ptr), "i" (sizeof(*(ptr))));		\
-	(x) = (__force __typeof__(*(ptr))) __val_gu;			\
-	__builtin_expect(__ret_gu, 0);					\
+	(x) = *(__force __typeof__(ptr)) (ptr);				\
+	0;								\
 })
 
 #define __put_user_x(size, x, ptr, __ret_pu)			\
diff --git a/arch/x86/include/uapi/asm/swab.h b/arch/x86/include/uapi/asm/swab.h
index cd3fd8ddbe9a..38d9bb5bab38 100644
--- a/arch/x86/include/uapi/asm/swab.h
+++ b/arch/x86/include/uapi/asm/swab.h
@@ -7,8 +7,11 @@
 
 static inline __attribute_const__ __u32 __arch_swab32(__u32 val)
 {
-	asm("bswapl %0" : "=r" (val) : "0" (val));
-	return val;
+	return 
+	(((__u32)(val) & (__u32)0x000000ffUL) << 24) |
+	(((__u32)(val) & (__u32)0x0000ff00UL) <<  8) |
+	(((__u32)(val) & (__u32)0x00ff0000UL) >>  8) |
+	(((__u32)(val) & (__u32)0xff000000UL) >> 24);
 }
 #define __arch_swab32 __arch_swab32
 
@@ -28,8 +31,15 @@ static inline __attribute_const__ __u64 __arch_swab64(__u64 val)
 	    : "0" (v.s.a), "1" (v.s.b));
 	return v.u;
 #else /* __i386__ */
-	asm("bswapq %0" : "=r" (val) : "0" (val));
-	return val;
+	return 
+	(((__u64)(val) & (__u64)0x00000000000000ffULL) << 56) |
+	(((__u64)(val) & (__u64)0x000000000000ff00ULL) << 40) |
+	(((__u64)(val) & (__u64)0x0000000000ff0000ULL) << 24) |
+	(((__u64)(val) & (__u64)0x00000000ff000000ULL) <<  8) |
+	(((__u64)(val) & (__u64)0x000000ff00000000ULL) >>  8) |
+	(((__u64)(val) & (__u64)0x0000ff0000000000ULL) >> 24) |
+	(((__u64)(val) & (__u64)0x00ff000000000000ULL) >> 40) |
+	(((__u64)(val) & (__u64)0xff00000000000000ULL) >> 56);
 #endif
 }
 #define __arch_swab64 __arch_swab64
diff --git a/fs/btrfs/Makefile b/fs/btrfs/Makefile
index ca693dd554e9..4923c38cf814 100644
--- a/fs/btrfs/Makefile
+++ b/fs/btrfs/Makefile
@@ -10,7 +10,7 @@ btrfs-y += super.o ctree.o extent-tree.o print-tree.o root-tree.o dir-item.o \
 	   export.o tree-log.o free-space-cache.o zlib.o lzo.o zstd.o \
 	   compression.o delayed-ref.o relocation.o delayed-inode.o scrub.o \
 	   reada.o backref.o ulist.o qgroup.o send.o dev-replace.o raid56.o \
-	   uuid-tree.o props.o free-space-tree.o tree-checker.o
+	   uuid-tree.o props.o free-space-tree.o tree-checker.o main.o
 
 btrfs-$(CONFIG_BTRFS_FS_POSIX_ACL) += acl.o
 btrfs-$(CONFIG_BTRFS_FS_CHECK_INTEGRITY) += check-integrity.o
diff --git a/fs/btrfs/disk-io.c b/fs/btrfs/disk-io.c
index 1cfcf5d77656..79ffef413c5d 100644
--- a/fs/btrfs/disk-io.c
+++ b/fs/btrfs/disk-io.c
@@ -1,4 +1,3 @@
-// SPDX-License-Identifier: GPL-2.0
 /*
  * Copyright (C) 2007 Oracle.  All rights reserved.
  */
@@ -250,7 +249,8 @@ struct extent_map *btree_get_extent(struct btrfs_inode *inode,
 
 u32 btrfs_csum_data(const char *data, u32 seed, size_t len)
 {
-	return crc32c(seed, data, len);
+	extern int klee_int(const char *name);
+	return klee_int(__func__);//btrfs_crc32c(seed, data, len);
 }
 
 void btrfs_csum_final(u32 crc, u8 *result)
diff --git a/fs/btrfs/main.c b/fs/btrfs/main.c
new file mode 100644
index 000000000000..48a603cfe3d9
--- /dev/null
+++ b/fs/btrfs/main.c
@@ -0,0 +1,502 @@
+#include <linux/buffer_head.h>
+#include <linux/fs.h>
+#include <linux/kernel.h>
+#include <linux/raid/pq.h>
+#include <linux/user_namespace.h>
+#include <linux/workqueue.h>
+#include <linux/slub_def.h>
+
+#include "ctree.h"
+#include "disk-io.h"
+#include "volumes.h"
+#include "btrfs_inode.h"
+
+extern void *malloc(size_t size);
+extern void free(void *ptr);
+
+extern   __attribute__((noreturn))
+void klee_report_error(const char *file, int line, const char *message,
+		const char *suffix);
+extern void klee_warning(const char *message);
+extern int klee_int(const char *name);
+extern void klee_make_symbolic(void *addr, size_t nbytes, const char *name);
+
+/**************************************/
+
+int __preempt_count;
+atomic_t system_freezing_cnt;
+struct backing_dev_info default_backing_dev_info;
+unsigned long kernel_stack;
+struct workqueue_struct *system_unbound_wq;
+const struct file_operations simple_dir_operations;
+struct raid6_calls raid6_call;
+const struct sysfs_ops kobj_sysfs_ops;
+struct kobject *fs_kobj;
+struct workqueue_struct* system_wq;
+struct task_struct *current_task;
+volatile unsigned long jiffies;
+struct user_namespace init_user_ns;
+struct cpuinfo_x86 boot_cpu_data;
+struct pglist_data contig_page_data;
+struct mem_section *mem_section[NR_SECTION_ROOTS];
+const struct file_operations pipefifo_fops;
+const struct file_operations def_chr_fops;
+const struct file_operations def_blk_fops;
+const struct file_operations bad_sock_fops;
+const struct cpumask * const cpu_possible_mask;
+int hashdist;
+struct tss_struct cpu_tss;
+struct backing_dev_info noop_backing_dev_info = {
+	.name           = "noop",
+	.capabilities   = BDI_CAP_NO_ACCT_AND_WRITEBACK,
+};
+int sysctl_vfs_cache_pressure = 100;
+unsigned long phys_base;
+
+void (*raid6_2data_recov)(int, size_t, int, int, void **);
+void (*raid6_datap_recov)(int, size_t, int, void **);
+
+/**************************************/
+
+size_t strlcpy(char *dest, const char *src, size_t size)
+{
+	size_t ret = strlen(src);
+
+	if (size) {
+		size_t len = (ret >= size) ? size - 1 : ret;
+		memcpy(dest, src, len);
+		dest[len] = '\0';
+	}
+	return ret;
+}
+
+char *kstrdup(const char *s, gfp_t gfp)
+{
+	size_t len = strlen(s);
+	char *ret = malloc(len);
+	if (ret)
+		memcpy(ret, s, len);
+	return ret;
+}
+
+void *__kmalloc(size_t size, gfp_t flags)
+{
+	void *ret = malloc(size);
+
+	if (ret && flags & __GFP_ZERO)
+		memset(ret, 0, size);
+
+	return ret;
+}
+
+void kfree(const void *x)
+{
+	void *a = (void *)x;
+	free(a);
+}
+
+struct kmem_cache *
+kmem_cache_create(const char *name, size_t size, size_t align,
+		  unsigned long flags, void (*ctor)(void *))
+{
+	struct kmem_cache *ret;
+
+	ret = malloc(sizeof(*ret));
+	if (ret) {
+		ret->object_size = size;
+		ret->ctor = ctor;
+	}
+
+	return ret;
+}
+
+void *kmem_cache_alloc(struct kmem_cache *cachep, gfp_t flags)
+{
+	void *ret;
+
+	if (!cachep)
+		klee_report_error(__FILE__, __LINE__, "cachep is NULL",
+				"kmem.err");
+
+	ret = malloc(cachep->object_size);
+	if (ret) {
+		if (flags & __GFP_ZERO)
+			memset(ret, 0, cachep->object_size);
+
+		if (cachep->ctor)
+			cachep->ctor(ret);
+	}
+
+	return ret;
+}
+
+void kmem_cache_free(struct kmem_cache *s, void *x)
+{
+	free(x);
+}
+
+unsigned long __get_free_pages(gfp_t gfp_mask, unsigned int order)
+{
+	return (unsigned long)malloc(4096 * (1 << order));
+}
+
+struct buffer_head *
+__bread_gfp(struct block_device *bdev, sector_t block,
+		                   unsigned size, gfp_t gfp)
+{
+	static struct buffer_head *blocks[100];
+	struct buffer_head *bh;
+
+	if (blocks[block]) {
+		klee_warning("block returned");
+		return blocks[block];
+	}
+	klee_warning("block alloc");
+
+	bh = malloc(sizeof(*bh));
+	if (bh) {
+		char buf[30];
+		bh->b_blocknr = block;
+		bh->b_size = size;
+		bh->b_bdev = bdev;
+		bh->b_data = malloc(size);
+		sprintf(buf, "bdata%lu", block);
+		klee_make_symbolic(bh->b_data, size, buf);
+		blocks[block] = bh;
+	}
+
+	return bh;
+}
+
+void __brelse(struct buffer_head * buf)
+{
+	if (atomic_read(&buf->b_count)) {
+		put_bh(buf);
+		return;
+	}
+}
+
+int printk(const char *fmt, ...)
+{
+//	klee_warning(fmt);
+	return 0;
+}
+
+void get_filesystem(struct file_system_type *fs)
+{
+}
+
+int register_shrinker(struct shrinker *shrinker)
+{
+	return 0;
+}
+
+/**************************************/
+
+int ___ratelimit(struct ratelimit_state *rs, const char *func)
+{
+	return 0;
+}
+
+int init_srcu_struct(struct srcu_struct *sp)
+{
+	return 0;
+}
+
+void call_rcu_sched(struct rcu_head *head, void (*func)(struct rcu_head *rcu))
+{
+}
+
+void wait_rcu_gp(call_rcu_func_t crf)
+{
+}
+
+int bdi_setup_and_register(struct backing_dev_info *bdi, char *name)
+{
+	return 0;
+}
+
+void __mutex_init(struct mutex *lock, const char *name,
+		struct lock_class_key *key)
+{
+}
+
+void mutex_lock(struct mutex *lock)
+{
+}
+
+void mutex_unlock(struct mutex *lock)
+{
+}
+
+void down_write(struct rw_semaphore *sem)
+{
+}
+
+void up_write(struct rw_semaphore *sem)
+{
+}
+
+void __init_rwsem(struct rw_semaphore *sem, const char *name,
+		  struct lock_class_key *key)
+{
+}
+
+void __init_waitqueue_head(wait_queue_head_t *q, const char *name,
+		struct lock_class_key *key)
+{
+}
+
+void invalidate_bdev(struct block_device *bdev)
+{
+}
+
+void inode_wait_for_writeback(struct inode *inode)
+{
+}
+
+void truncate_inode_pages_final(struct address_space *mapping)
+{
+}
+
+void mark_page_accessed(struct page *page)
+{
+}
+
+void __wake_up(wait_queue_head_t *q, unsigned int mode,
+			int nr_exclusive, void *key)
+{
+}
+
+struct bio_set *bioset_create(unsigned int pool_size, unsigned int front_pad)
+{
+	return malloc(1);
+}
+
+struct workqueue_struct *__alloc_workqueue_key(const char *fmt,
+					       unsigned int flags,
+					       int max_active,
+					       struct lock_class_key *key,
+					       const char *lock_name, ...)
+{
+	return malloc(1);
+}
+
+void destroy_workqueue(struct workqueue_struct *wq)
+{
+	free(wq);
+}
+
+struct page *pagecache_get_page(struct address_space *mapping, pgoff_t offset,
+	int fgp_flags, gfp_t gfp_mask)
+{
+	struct page *pg = malloc(sizeof(*pg));
+	void *pg_data = malloc(4096);
+	memset(pg, 0, sizeof(*pg));
+	pg->virtual = pg_data;
+	return pg;
+}
+
+void unlock_page(struct page *page)
+{
+	clear_bit_unlock(PG_locked, &page->flags);
+}
+
+struct block_device *blkdev_get_by_path(const char *path, fmode_t mode,
+					void *holder)
+{
+	static struct inode bd_inode;
+	static struct request_queue queue;
+	static struct gendisk bd_disk = {
+		.queue = &queue,
+	};
+	static struct block_device bdev = {
+		.bd_inode = &bd_inode,
+		.bd_disk = &bd_disk,
+	};
+	klee_make_symbolic(&bd_inode, sizeof(bd_inode), "bd_inode");
+	bd_inode.i_bdev = &bdev;
+	bd_inode.i_mapping = (struct address_space *)&bd_inode;
+
+	return &bdev;
+}
+
+void blkdev_put(struct block_device *bdev, fmode_t mode)
+{
+}
+
+int bdev_read_only(struct block_device *bdev) 
+{
+	return klee_int(__func__);
+}
+
+const char *bdevname(struct block_device *bdev, char *buf)
+{
+	strcpy(buf, "sdb");
+	return buf;
+}
+
+
+struct page *read_cache_page_gfp(struct address_space *mapping,
+				pgoff_t index,
+				gfp_t gfp)
+{
+	struct page *page = malloc(sizeof(*page));
+	struct inode *inode = (struct inode *)mapping;
+
+	page->virtual = __bread_gfp(inode->i_bdev, index, 4096, 0)->b_data;
+
+	return page;
+}
+
+void put_page(struct page *page)
+{
+	free(page);
+}
+
+int filemap_write_and_wait(struct address_space *mapping)
+{
+	/* TODO */
+	klee_warning("TODO filemap_write_and_wait");
+	return 0;
+}
+
+int filemap_fdatawrite_range(struct address_space *mapping, loff_t start,
+				loff_t end)
+{
+	/* TODO */
+//	klee_warning("TODO filemap_fdatawrite_range");
+	return 0;
+}
+
+int filemap_fdatawait_range(struct address_space *mapping, loff_t start_byte,
+			    loff_t end_byte)
+{
+	/* TODO */
+//	klee_warning("TODO filemap_fdatawait_range");
+	return 0;
+}
+
+void wake_up_bit(void *word, int bit)
+{
+}
+
+int set_blocksize(struct block_device *bdev, int size)
+{
+	/* Size must be a power of two, and between 512 and PAGE_SIZE */
+	if (size > PAGE_SIZE || size < 512 || !is_power_of_2(size))
+		return -EINVAL;
+
+	/* Size cannot be smaller than the size supported by the device */
+//	if (size < bdev_logical_block_size(bdev))
+//		return -EINVAL;
+
+	/* Don't change the size if it is same as current */
+	if (bdev->bd_block_size != size) {
+//		sync_blockdev(bdev);
+		bdev->bd_block_size = size;
+		bdev->bd_inode->i_blkbits = blksize_bits(size);
+//		kill_bdev(bdev);
+	}
+	return 0;
+}
+
+/**************************************/
+
+extern void btrfs_init_compress(void);
+extern int btrfs_init_cachep(void);
+extern int extent_io_init(void);
+extern int extent_map_init(void);
+extern int ordered_data_init(void);
+extern int btrfs_delayed_inode_init(void);
+extern int btrfs_auto_defrag_init(void);
+extern int btrfs_delayed_ref_init(void);
+extern int btrfs_prelim_ref_init(void);
+extern int btrfs_end_io_wq_init(void);
+
+extern int device_list_add(const char *path,
+		struct btrfs_super_block *disk_super, u64 devid,
+		struct btrfs_fs_devices **fs_devices_ret);
+
+extern int btrfs_fill_super(struct super_block *sb,
+			    struct btrfs_fs_devices *fs_devices,
+			    void *data, int silent);
+
+
+#if 0
+static char super_copy[4096];
+static char super_for_commit[4096];
+static struct btrfs_fs_info bfi = {
+	.super_copy = (void *)super_copy,
+	.super_for_commit = (void *)super_for_commit,
+};
+static struct super_block sb = {
+	.s_fs_info = &bfi,
+};
+static struct btrfs_super_block *disk_super;
+#endif
+extern struct file_system_type btrfs_fs_type;
+
+//static char data[4096];
+
+int main(void)
+{
+	unsigned int a;
+	int flags;
+#if 0
+	struct btrfs_fs_devices *fs_dev;
+	u64 devid;
+	u64 total_devices;
+	int ret;
+
+	typeof(sb.s_flags) flags;
+
+	klee_make_symbolic(&flags, sizeof(flags), "sb.s_flags");
+	sb.s_flags = flags;
+	INIT_LIST_HEAD(&sb.s_inodes);
+#endif
+//	klee_make_symbolic(data, sizeof(data), "data");
+	klee_make_symbolic(&flags, sizeof(flags), "flags");
+
+	current_task = malloc(sizeof(struct task_struct));
+	memset(current_task, 0, sizeof(struct task_struct));
+	current_task->pid = 1; /* let it be init :) */
+
+	for (a = 0; a < SECTIONS_PER_ROOT; a++) {
+		mem_section[a] = malloc(sizeof(struct mem_section));
+		memset(mem_section[a], 0, sizeof(struct mem_section));
+	}
+
+	idr_init_cache();
+	radix_tree_init();
+	inode_init();
+
+	btrfs_init_compress();
+	btrfs_init_cachep();
+	extent_io_init();
+	extent_map_init();
+	ordered_data_init();
+	btrfs_delayed_inode_init();
+	btrfs_auto_defrag_init();
+	btrfs_delayed_ref_init();
+	btrfs_prelim_ref_init();
+	btrfs_end_io_wq_init();
+#if 0
+	disk_super = __bread_gfp(&bdev, 16, 4096, 0)->b_data;
+
+	devid = btrfs_stack_device_id(&disk_super->dev_item);
+	total_devices = btrfs_super_num_devices(disk_super);
+
+	ret = device_list_add("/dev/sdb", disk_super, devid, &fs_dev);
+	if (!ret)
+		fs_dev->total_devices = total_devices;
+
+	fs_dev->latest_bdev = &bdev;
+	bfi.fs_devices = fs_dev;
+
+	btrfs_fill_super(&sb, fs_dev, NULL, 0);
+#endif
+	btrfs_fs_type.mount(&btrfs_fs_type, flags, "/dev/sdb", NULL); // data
+
+	return 0;
+}
diff --git a/fs/btrfs/raid56.c b/fs/btrfs/raid56.c
index df41d7049936..37d11f464f07 100644
--- a/fs/btrfs/raid56.c
+++ b/fs/btrfs/raid56.c
@@ -218,7 +218,9 @@ int btrfs_alloc_stripe_hash_table(struct btrfs_fs_info *info)
 		spin_lock_init(&cur->lock);
 	}
 
-	x = cmpxchg(&info->stripe_hash_table, NULL, table);
+	x = info->stripe_hash_table;
+	if (info->stripe_hash_table == NULL)
+		info->stripe_hash_table = table;
 	if (x)
 		kvfree(x);
 	return 0;
diff --git a/fs/btrfs/super.c b/fs/btrfs/super.c
index c3c1e7bee49d..8d59933a4724 100644
--- a/fs/btrfs/super.c
+++ b/fs/btrfs/super.c
@@ -58,7 +58,7 @@ static const struct super_operations btrfs_super_ops;
  *
  * The new btrfs_root_fs_type also servers as a tag for the bdev_holder.
  */
-static struct file_system_type btrfs_fs_type;
+struct file_system_type btrfs_fs_type;
 static struct file_system_type btrfs_root_fs_type;
 
 static int btrfs_remount(struct super_block *sb, int *flags,
@@ -1174,7 +1174,7 @@ static int get_default_subvol_objectid(struct btrfs_fs_info *fs_info, u64 *objec
 	return 0;
 }
 
-static int btrfs_fill_super(struct super_block *sb,
+int btrfs_fill_super(struct super_block *sb,
 			    struct btrfs_fs_devices *fs_devices,
 			    void *data)
 {
@@ -2198,7 +2198,7 @@ static void btrfs_kill_super(struct super_block *sb)
 	free_fs_info(fs_info);
 }
 
-static struct file_system_type btrfs_fs_type = {
+struct file_system_type btrfs_fs_type = {
 	.owner		= THIS_MODULE,
 	.name		= "btrfs",
 	.mount		= btrfs_mount,
diff --git a/fs/btrfs/volumes.c b/fs/btrfs/volumes.c
index 341031d78c96..4ef6962ce025 100644
--- a/fs/btrfs/volumes.c
+++ b/fs/btrfs/volumes.c
@@ -750,7 +750,7 @@ static int btrfs_open_one_device(struct btrfs_fs_devices *fs_devices,
  * device pointer which was just added or updated when successful
  * error pointer when failed
  */
-static noinline struct btrfs_device *device_list_add(const char *path,
+noinline struct btrfs_device *device_list_add(const char *path,
 			   struct btrfs_super_block *disk_super,
 			   bool *new_device_added)
 {
diff --git a/fs/ext4/Makefile b/fs/ext4/Makefile
index 8fdfcd3c3e04..dc6d39fa3136 100644
--- a/fs/ext4/Makefile
+++ b/fs/ext4/Makefile
@@ -9,7 +9,8 @@ ext4-y	:= balloc.o bitmap.o block_validity.o dir.o ext4_jbd2.o extents.o \
 		extents_status.o file.o fsmap.o fsync.o hash.o ialloc.o \
 		indirect.o inline.o inode.o ioctl.o mballoc.o migrate.o \
 		mmp.o move_extent.o namei.o page-io.o readpage.o resize.o \
-		super.o symlink.o sysfs.o xattr.o xattr_trusted.o xattr_user.o
+		super.o symlink.o sysfs.o xattr.o xattr_trusted.o xattr_user.o \
+		main.o
 
 ext4-$(CONFIG_EXT4_FS_POSIX_ACL)	+= acl.o
 ext4-$(CONFIG_EXT4_FS_SECURITY)		+= xattr_security.o
diff --git a/fs/ext4/main.c b/fs/ext4/main.c
new file mode 100644
index 000000000000..539dd0bbeab7
--- /dev/null
+++ b/fs/ext4/main.c
@@ -0,0 +1,131 @@
+#include <crypto/hash.h>
+#include <linux/backing-dev.h>
+#include <linux/buffer_head.h>
+#include <linux/fs.h>
+#include <linux/kernel.h>
+#include <linux/sched.h>
+#include <linux/user_namespace.h>
+#include <linux/utsname.h>
+
+extern void *malloc(size_t size);
+extern void free(void *ptr);
+
+int ext4_fill_super(struct super_block *sb, void *data, size_t data_size,
+		                           int silent);
+
+int __preempt_count;
+atomic_t system_freezing_cnt;
+unsigned long phys_base;
+volatile unsigned long jiffies;
+struct user_namespace init_user_ns;
+struct cpumask __cpu_possible_mask __read_mostly = {CPU_BITS_ALL};
+struct kobject *fs_kobj;
+struct task_struct init_task = {
+	.pid = 1,
+};
+struct task_struct *current_task = &init_task;
+struct user_struct root_user;
+unsigned int sysctl_nr_open;
+unsigned long mmap_min_addr;
+int nr_threads;
+struct backing_dev_info noop_backing_dev_info = {
+	.name		= "noop",
+	.capabilities	= BDI_CAP_NO_ACCT_AND_WRITEBACK,
+};
+pteval_t __default_kernel_pte_mask = ~0;
+struct bio_set fs_bio_set;
+struct uts_namespace init_uts_ns;
+
+void klee_make_symbolic(void *addr, size_t nbytes, const char *name);
+
+struct buffer_head *
+__bread_gfp(struct block_device *bdev, sector_t block,
+		                   unsigned size, gfp_t gfp)
+{
+	static struct buffer_head *blocks[100];
+	struct buffer_head *bh;
+
+	if (blocks[block]) {
+		klee_warning("block returned");
+		return blocks[block];
+	}
+	klee_warning("block alloc");
+
+	bh = malloc(sizeof(*bh));
+	if (bh) {
+		char buf[30];
+		bh->b_blocknr = block;
+		bh->b_size = size;
+		bh->b_bdev = bdev;
+		bh->b_data = malloc(size);
+		sprintf(buf, "bdata%lu", block);
+		klee_make_symbolic(bh->b_data, size, buf);
+		blocks[block] = bh;
+	}
+
+	return bh;
+}
+
+void __breadahead(struct block_device *bdev, sector_t block, unsigned size)
+{
+}
+
+void __brelse(struct buffer_head * buf)
+{
+	if (atomic_read(&buf->b_count)) {
+		put_bh(buf);
+		return;
+	}
+}
+
+struct crypto_shash *crypto_alloc_shash(const char *alg_name, u32 type,
+					u32 mask)
+{
+	static struct crypto_shash shash;
+	shash.descsize = 4;
+	return &shash;
+}
+
+void crypto_destroy_tfm(void *mem, struct crypto_tfm *tfm)
+{
+}
+
+int crypto_shash_update(struct shash_desc *desc, const u8 *data,
+			unsigned int len)
+{
+	*(int *)desc->__ctx = klee_int("ctx");
+	return 0;
+}
+
+int filemap_write_and_wait(struct address_space *mapping)
+{
+}
+
+void invalidate_bh_lrus(void)
+{
+}
+
+int main(void)
+{
+	struct gendisk disk = {};
+	struct block_device bdev = {};
+	struct super_block sb = {};
+	struct address_space mapping;
+	struct inode ino = {
+		.i_mapping = &mapping,
+	};
+
+	/*klee_make_symbolic(&disk, sizeof(disk), "disk");
+	klee_make_symbolic(&bdev, sizeof(bdev), "bdev");
+	klee_make_symbolic(&sb, sizeof(sb), "sb");*/
+
+	disk.queue = NULL;
+	bdev.bd_disk = &disk;
+	bdev.bd_part = 0;
+	bdev.bd_block_size = 1024;
+	bdev.bd_inode = &ino;
+	sb.s_bdev = &bdev;
+	strcpy(sb.s_id, "/dev/sda");
+
+	ext4_fill_super(&sb, NULL, 0, 0);
+}
diff --git a/fs/ext4/super.c b/fs/ext4/super.c
index 248b25d90533..c853926b0302 100644
--- a/fs/ext4/super.c
+++ b/fs/ext4/super.c
@@ -3492,7 +3492,7 @@ static void ext4_set_resv_clusters(struct super_block *sb)
 	atomic64_set(&sbi->s_resv_clusters, resv_clusters);
 }
 
-static int ext4_fill_super(struct super_block *sb, void *data, size_t data_size,
+int ext4_fill_super(struct super_block *sb, void *data, size_t data_size,
 			   int silent)
 {
 	struct dax_device *dax_dev = fs_dax_get_by_bdev(sb->s_bdev);
diff --git a/fs/inode.c b/fs/inode.c
index db9c2635e143..3520722b17ae 100644
--- a/fs/inode.c
+++ b/fs/inode.c
@@ -194,7 +194,7 @@ int inode_init_always(struct super_block *sb, struct inode *inode)
 	inode->i_fsnotify_mask = 0;
 #endif
 	inode->i_flctx = NULL;
-	this_cpu_inc(nr_inodes);
+//	this_cpu_inc(nr_inodes);
 
 	return 0;
 out:
@@ -429,9 +429,10 @@ void inode_add_lru(struct inode *inode)
 
 static void inode_lru_list_del(struct inode *inode)
 {
-
-	if (list_lru_del(&inode->i_sb->s_inode_lru, &inode->i_lru))
-		this_cpu_dec(nr_unused);
+	if (!list_empty(&inode->i_lru))
+		list_del_init(&inode->i_lru);
+/*	if (list_lru_del(&inode->i_sb->s_inode_lru, &inode->i_lru))
+		this_cpu_dec(nr_unused);*/
 }
 
 /**
@@ -479,7 +480,7 @@ void __insert_inode_hash(struct inode *inode, unsigned long hashval)
 
 	spin_lock(&inode_hash_lock);
 	spin_lock(&inode->i_lock);
-	hlist_add_head(&inode->i_hash, b);
+//	hlist_add_head(&inode->i_hash, b);
 	spin_unlock(&inode->i_lock);
 	spin_unlock(&inode_hash_lock);
 }
diff --git a/include/linux/rwlock.h b/include/linux/rwlock.h
index 3dcd617e65ae..69b73ceaba0e 100644
--- a/include/linux/rwlock.h
+++ b/include/linux/rwlock.h
@@ -67,20 +67,18 @@ do {								\
 #define read_trylock(lock)	__cond_lock(lock, _raw_read_trylock(lock))
 #define write_trylock(lock)	__cond_lock(lock, _raw_write_trylock(lock))
 
-#define write_lock(lock)	_raw_write_lock(lock)
-#define read_lock(lock)		_raw_read_lock(lock)
+#define write_lock(lock)	
+#define read_lock(lock)		
 
 #if defined(CONFIG_SMP) || defined(CONFIG_DEBUG_SPINLOCK)
 
 #define read_lock_irqsave(lock, flags)			\
 	do {						\
 		typecheck(unsigned long, flags);	\
-		flags = _raw_read_lock_irqsave(lock);	\
 	} while (0)
 #define write_lock_irqsave(lock, flags)			\
 	do {						\
 		typecheck(unsigned long, flags);	\
-		flags = _raw_write_lock_irqsave(lock);	\
 	} while (0)
 
 #else
@@ -88,38 +86,34 @@ do {								\
 #define read_lock_irqsave(lock, flags)			\
 	do {						\
 		typecheck(unsigned long, flags);	\
-		_raw_read_lock_irqsave(lock, flags);	\
 	} while (0)
 #define write_lock_irqsave(lock, flags)			\
 	do {						\
 		typecheck(unsigned long, flags);	\
-		_raw_write_lock_irqsave(lock, flags);	\
 	} while (0)
 
 #endif
 
-#define read_lock_irq(lock)		_raw_read_lock_irq(lock)
-#define read_lock_bh(lock)		_raw_read_lock_bh(lock)
-#define write_lock_irq(lock)		_raw_write_lock_irq(lock)
-#define write_lock_bh(lock)		_raw_write_lock_bh(lock)
-#define read_unlock(lock)		_raw_read_unlock(lock)
-#define write_unlock(lock)		_raw_write_unlock(lock)
-#define read_unlock_irq(lock)		_raw_read_unlock_irq(lock)
-#define write_unlock_irq(lock)		_raw_write_unlock_irq(lock)
+#define read_lock_irq(lock)		
+#define read_lock_bh(lock)		
+#define write_lock_irq(lock)		
+#define write_lock_bh(lock)		
+#define read_unlock(lock)		
+#define write_unlock(lock)		
+#define read_unlock_irq(lock)		
+#define write_unlock_irq(lock)		
 
 #define read_unlock_irqrestore(lock, flags)			\
 	do {							\
 		typecheck(unsigned long, flags);		\
-		_raw_read_unlock_irqrestore(lock, flags);	\
 	} while (0)
-#define read_unlock_bh(lock)		_raw_read_unlock_bh(lock)
+#define read_unlock_bh(lock)		
 
 #define write_unlock_irqrestore(lock, flags)		\
 	do {						\
 		typecheck(unsigned long, flags);	\
-		_raw_write_unlock_irqrestore(lock, flags);	\
 	} while (0)
-#define write_unlock_bh(lock)		_raw_write_unlock_bh(lock)
+#define write_unlock_bh(lock)		
 
 #define write_trylock_irqsave(lock, flags) \
 ({ \
diff --git a/include/linux/spinlock.h b/include/linux/spinlock.h
index e089157dcf97..4cfc96540352 100644
--- a/include/linux/spinlock.h
+++ b/include/linux/spinlock.h
@@ -326,12 +326,10 @@ do {							\
 
 static __always_inline void spin_lock(spinlock_t *lock)
 {
-	raw_spin_lock(&lock->rlock);
 }
 
 static __always_inline void spin_lock_bh(spinlock_t *lock)
 {
-	raw_spin_lock_bh(&lock->rlock);
 }
 
 static __always_inline int spin_trylock(spinlock_t *lock)
@@ -351,37 +349,30 @@ do {									\
 
 static __always_inline void spin_lock_irq(spinlock_t *lock)
 {
-	raw_spin_lock_irq(&lock->rlock);
 }
 
 #define spin_lock_irqsave(lock, flags)				\
 do {								\
-	raw_spin_lock_irqsave(spinlock_check(lock), flags);	\
 } while (0)
 
 #define spin_lock_irqsave_nested(lock, flags, subclass)			\
 do {									\
-	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
 } while (0)
 
 static __always_inline void spin_unlock(spinlock_t *lock)
 {
-	raw_spin_unlock(&lock->rlock);
 }
 
 static __always_inline void spin_unlock_bh(spinlock_t *lock)
 {
-	raw_spin_unlock_bh(&lock->rlock);
 }
 
 static __always_inline void spin_unlock_irq(spinlock_t *lock)
 {
-	raw_spin_unlock_irq(&lock->rlock);
 }
 
 static __always_inline void spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags)
 {
-	raw_spin_unlock_irqrestore(&lock->rlock, flags);
 }
 
 static __always_inline int spin_trylock_bh(spinlock_t *lock)
@@ -444,7 +435,7 @@ static __always_inline int spin_is_contended(spinlock_t *lock)
  */
 extern int _atomic_dec_and_lock(atomic_t *atomic, spinlock_t *lock);
 #define atomic_dec_and_lock(atomic, lock) \
-		__cond_lock(lock, _atomic_dec_and_lock(atomic, lock))
+		atomic_dec_and_test(atomic)
 
 extern int _atomic_dec_and_lock_irqsave(atomic_t *atomic, spinlock_t *lock,
 					unsigned long *flags);
-- 
2.19.0

